{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/features/')\n",
    "from custom_metric import *\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b100db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define noise levels\n",
    "\n",
    "levels = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaca03",
   "metadata": {},
   "source": [
    "## Random Forest 1 - Multiclass\n",
    "#### Features = 'capacity', 'vib1', 'vib2', 'amp_uni', 'mic1', 'mic2'\n",
    "#### Value = 'type_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef546b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Unnamed: 0', 'datetime', 'date', 'time', 'pump', 'pump_cat', 'capacity_cat', 'anomaly', 'anomaly_cat', 'anomaly_binary',\n",
    "       'anomaly_binary_cat', 'type', 'vib1', 'vib1_x', 'vib1_y',\n",
    "       'vib1_z', 'vib2', 'vib2_x', 'vib2_y', 'vib2_z', 'amp1', 'amp2',\n",
    "       'amp_uni', 'mic1', 'mic2',\n",
    "       'vib1_x_noise', 'vib1_x_with_noise', 'vib1_y_noise',\n",
    "       'vib1_y_with_noise', 'vib1_z_noise', 'vib1_z_with_noise', 'vib2_x_noise', 'vib2_x_with_noise', 'vib2_y_noise',\n",
    "       'vib2_y_with_noise', 'vib2_z_noise', 'vib2_z_with_noise', 'amp1_noise',\n",
    "       'amp1_with_noise', 'amp2_noise', 'amp2_with_noise', 'mic1_noise', 'mic2_noise']\n",
    "\n",
    "value = 'type_cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c010039",
   "metadata": {},
   "source": [
    "### Firstly, let's just train a model previosly hypertuned for clean data and see how the metrics change with change in noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "custom_metrics = {}\n",
    "\n",
    "for level in levels:\n",
    "    \n",
    "    #read in data\n",
    "    data = pd.read_csv('../data/noisy_data_'+str(level)+'.csv')\n",
    "    \n",
    "    #drop the columns we don't need\n",
    "    df = data.drop(drop_list, axis=1)\n",
    "    \n",
    "    #define features and value\n",
    "    X = df.drop(value, axis=1)\n",
    "    y = df[value]\n",
    "    \n",
    "    #create train and test datasets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 100)\n",
    "    \n",
    "    #train the model\n",
    "    model = RandomForestClassifier(random_state = 100, bootstrap = 'False', min_samples_split=5, max_features=2, n_estimators = 35)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #calculate accuracy and custom metric\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=100)\n",
    "    scores1 = cross_val_score(model, X, y, cv=cv, n_jobs=1)\n",
    "    accuracies[level] = scores1\n",
    "    \n",
    "    scores2 = cross_val_score(model, X, y, cv=cv, n_jobs=1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "    custom_metrics[level] = scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = accuracies.keys()\n",
    "mean_accuracies = [accuracies[n].mean() for n in levels]\n",
    "# show range of scores using min and max values\n",
    "lower_lim = [accuracies[n].min() for n in levels]\n",
    "upper_lim = [accuracies[n].max() for n in levels]\n",
    "\n",
    "plt.plot(noise_levels, mean_accuracies)\n",
    "plt.fill_between(noise_levels, lower_lim, upper_lim, alpha=0.3, color='gray')\n",
    "\n",
    "plt.title(f'Random forest (for {value}) accuracies for different noise levels', fontsize=16)\n",
    "plt.xlabel('noise level', fontsize=14)\n",
    "plt.ylabel('CV accuracy score (mean score / range)', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig('random_forest_noise.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = custom_metrics.keys()\n",
    "mean_custom = [custom_metrics[n].mean() for n in levels]\n",
    "# show range of scores using min and max values\n",
    "lower_lim = [custom_metrics[n].min() for n in levels]\n",
    "upper_lim = [custom_metrics[n].max() for n in levels]\n",
    "\n",
    "plt.plot(noise_levels, mean_custom)\n",
    "plt.fill_between(noise_levels, lower_lim, upper_lim, alpha=0.3, color='gray')\n",
    "\n",
    "plt.title(f'Random forest (for {value}) custom metrics for different noise levels', fontsize=16)\n",
    "plt.xlabel('noise level', fontsize=14)\n",
    "plt.ylabel('CV custom metric score (mean score / range)', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "#plt.show()\n",
    "#plt.savefig('adaboost_noise.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in levels:\n",
    "    \n",
    "    print('Noise level: '+str(level)+'\\n')\n",
    "    print('Accuracy: '+ str(mean_accuracies[level-1])+'\\n')\n",
    "    print('Custom metric: '+ str(mean_custom[level-1])+'\\n')\n",
    "    print('**************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecade48",
   "metadata": {},
   "source": [
    "### Now, let's try hypertuning the models for each noise level separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f98767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's prepare the dataframes for all noise levels\n",
    "\n",
    "for level in levels:\n",
    "    \n",
    "    #read in data\n",
    "    globals()[f'data_{level}'] = pd.read_csv('../data/noisy_data_'+str(level)+'.csv')\n",
    "    \n",
    "    #drop the columns we don't need\n",
    "    globals()[f'df_{level}'] = globals()[f'data_{level}'].drop(drop_list, axis=1)\n",
    "    \n",
    "    #define features and value\n",
    "    globals()[f'X_{level}'] = globals()[f'df_{level}'].drop(value, axis=1)\n",
    "    globals()[f'y_{level}'] = globals()[f'df_{level}'][value]\n",
    "    \n",
    "    #scale the data\n",
    "    scaler = StandardScaler()  \n",
    "    globals()[f'X_{level}'] = scaler.fit_transform(globals()[f'X_{level}']) \n",
    "    \n",
    "    #create train and test datasets\n",
    "    globals()[f'X_{level}_train'], globals()[f'X_{level}_val'], globals()[f'y_{level}_train'], globals()[f'y_{level}_val'] = train_test_split(X, y, test_size = 0.20, random_state = 100)\n",
    "    \n",
    "\n",
    "# Define parameters for hypertuning\n",
    "\n",
    "n_estimators = [25, 35, 45]\n",
    "\n",
    "max_features = [2,4,6]\n",
    "\n",
    "min_samples_split = [10, 20, 40]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "weights = [{1: 1000, 2: 1000, 3: 1000},\n",
    "           {1: 3000, 2: 3000, 3: 3000},\n",
    "           {1: 5000, 2: 5000, 3: 5000},\n",
    "          {1: 10000, 2: 10000, 3: 10000}]\n",
    "\n",
    "max_depth = [5, 10, 15]\n",
    "\n",
    "# Create the grid\n",
    "search_params = {'n_estimators': n_estimators,\n",
    "                 'max_features': max_features,\n",
    "                 'min_samples_split': min_samples_split,\n",
    "                 'bootstrap': bootstrap,\n",
    "                 'criterion': criterion,\n",
    "                 'class_weight': weights,\n",
    "                 'max_depth': max_depth}\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175df134",
   "metadata": {},
   "source": [
    "### Noise level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_1 = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_1.fit(X_1_train, y_1_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_1_v2 = GridSearchCV(model_1, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "\n",
    "model_1_v2.fit(X_1_train, y_1_train)\n",
    "\n",
    "best_params = model_1_v2.best_params_\n",
    "\n",
    "print('Best Parameters', model_1_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_1_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 5000, 2: 5000, 3: 5000}, criterion='entropy', max_depth=10)\n",
    "model_1_v3.fit(X_1_train, y_1_train)\n",
    "\n",
    "scores = cross_val_score(model_1_v3, X_1, y_1, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_1_v3, X_1, y_1, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec98f4",
   "metadata": {},
   "source": [
    "### Noise level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_2 = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_2.fit(X_2_train, y_2_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_2_v2 = GridSearchCV(model_2, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "\n",
    "model_2_v2.fit(X_2_train, y_2_train)\n",
    "\n",
    "best_params = model_2_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_2_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_2_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 3000, 2: 3000, 3: 3000}, criterion='entropy', max_depth=10)\n",
    "model_2_v3.fit(X_2_train, y_2_train)\n",
    "\n",
    "scores = cross_val_score(model_2_v3, X_2, y_2, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_2_v3, X_2, y_2, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c8502",
   "metadata": {},
   "source": [
    "### Noise level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_3 = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_3.fit(X_3_train, y_3_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_3_v2 = GridSearchCV(model_3, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "\n",
    "model_3_v2.fit(X_3_train, y_3_train)\n",
    "\n",
    "best_params = model_3_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_3_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_3_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 3000, 2: 3000, 3: 3000}, criterion='entropy', max_depth=10)\n",
    "model_3_v3.fit(X_3_train, y_3_train)\n",
    "\n",
    "scores = cross_val_score(model_3_v3, X_3, y_3, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_3_v3, X_3, y_3, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596519b",
   "metadata": {},
   "source": [
    "## Random forests 2 - Binary\n",
    "#### Features = 'capacity', 'vib1', 'vib2', 'amp_uni', 'mic1', 'mic2'\n",
    "#### Value = 'anomaly_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96671db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list_b = ['Unnamed: 0', 'datetime', 'date', 'time', 'pump', 'pump_cat', 'capacity_cat', 'anomaly', 'anomaly_cat', 'type_cat',\n",
    "       'anomaly_binary_cat', 'type', 'vib1', 'vib1_x', 'vib1_y',\n",
    "       'vib1_z', 'vib2', 'vib2_x', 'vib2_y', 'vib2_z', 'amp1', 'amp2',\n",
    "       'amp_uni', 'mic1', 'mic2',\n",
    "       'vib1_x_noise', 'vib1_x_with_noise', 'vib1_y_noise',\n",
    "       'vib1_y_with_noise', 'vib1_z_noise', 'vib1_z_with_noise', 'vib2_x_noise', 'vib2_x_with_noise', 'vib2_y_noise',\n",
    "       'vib2_y_with_noise', 'vib2_z_noise', 'vib2_z_with_noise', 'amp1_noise',\n",
    "       'amp1_with_noise', 'amp2_noise', 'amp2_with_noise', 'mic1_noise', 'mic2_noise']\n",
    "\n",
    "value_b = 'anomaly_binary'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a57fe9",
   "metadata": {},
   "source": [
    "### Firstly, let's just train a model previosly hypertuned for clean data and see how the metrics change with change in noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "custom_metrics = {}\n",
    "\n",
    "for level in levels:\n",
    "    \n",
    "    #read in data\n",
    "    data = pd.read_csv('../data/noisy_data_'+str(level)+'.csv')\n",
    "    \n",
    "    #drop the columns we don't need\n",
    "    df = data.drop(drop_list_b, axis=1)\n",
    "    \n",
    "    #define features and value\n",
    "    X = df.drop(value_b, axis=1)\n",
    "    y = df[value_b]\n",
    "    \n",
    "    #create train and test datasets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 100)\n",
    "    \n",
    "    #train the model\n",
    "    model = RandomForestClassifier(random_state = 100, bootstrap = 'False', min_samples_split=5, max_features=2, n_estimators = 35)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #calculate accuracy and custom metric\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=100)\n",
    "    scores1 = cross_val_score(model, X, y, cv=cv, n_jobs=1)\n",
    "    accuracies[level] = scores1\n",
    "    \n",
    "    scores2 = cross_val_score(model, X, y, cv=cv, n_jobs=1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "    custom_metrics[level] = scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = accuracies.keys()\n",
    "mean_accuracies = [accuracies[n].mean() for n in levels]\n",
    "# show range of scores using min and max values\n",
    "lower_lim = [accuracies[n].min() for n in levels]\n",
    "upper_lim = [accuracies[n].max() for n in levels]\n",
    "\n",
    "plt.plot(noise_levels, mean_accuracies)\n",
    "plt.fill_between(noise_levels, lower_lim, upper_lim, alpha=0.3, color='gray')\n",
    "\n",
    "plt.title(f'Random forest (for {value_b}) accuracies for different noise levels', fontsize=16)\n",
    "plt.xlabel('noise level', fontsize=14)\n",
    "plt.ylabel('CV accuracy score (mean score / range)', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "#plt.show()\n",
    "#plt.savefig('random_forest_noise.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = custom_metrics.keys()\n",
    "mean_custom = [custom_metrics[n].mean() for n in levels]\n",
    "# show range of scores using min and max values\n",
    "lower_lim = [custom_metrics[n].min() for n in levels]\n",
    "upper_lim = [custom_metrics[n].max() for n in levels]\n",
    "\n",
    "plt.plot(noise_levels, mean_custom)\n",
    "plt.fill_between(noise_levels, lower_lim, upper_lim, alpha=0.3, color='gray')\n",
    "\n",
    "plt.title(f'Random forest (for {value_b}) custom metrics for different noise levels', fontsize=16)\n",
    "plt.xlabel('noise level', fontsize=14)\n",
    "plt.ylabel('CV custom metric score (mean score / range)', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "#plt.show()\n",
    "#plt.savefig('adaboost_noise.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in levels:\n",
    "    \n",
    "    print('Noise level: '+str(level)+'\\n')\n",
    "    print('Accuracy: '+ str(mean_accuracies[level-1])+'\\n')\n",
    "    print('Custom metric: '+ str(mean_custom[level-1])+'\\n')\n",
    "    print('**************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a863e0",
   "metadata": {},
   "source": [
    "### Now, let's try hypertuning the models for each noise level separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28526b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's prepare the dataframes for all noise levels\n",
    "\n",
    "for level in levels:\n",
    "    \n",
    "    #drop the columns we don't need\n",
    "    globals()[f'df_{level}_b'] = globals()[f'data_{level}'].drop(drop_list_b, axis=1)\n",
    "    \n",
    "    #define features and value\n",
    "    globals()[f'X_{level}_b'] = globals()[f'df_{level}_b'].drop(value_b, axis=1)\n",
    "    globals()[f'y_{level}_b'] = globals()[f'df_{level}_b'][value_b]\n",
    "    \n",
    "    #scale the data\n",
    "    scaler = StandardScaler()  \n",
    "    globals()[f'X_{level}_b'] = scaler.fit_transform(globals()[f'X_{level}_b']) \n",
    "    \n",
    "    #create train and test datasets\n",
    "    globals()[f'X_{level}_b_train'], globals()[f'X_{level}_b_val'], globals()[f'y_{level}_b_train'], globals()[f'y_{level}_b_val'] = train_test_split(globals()[f'X_{level}'], globals()[f'y_{level}'], test_size = 0.20, random_state = 100)\n",
    "    \n",
    "\n",
    "# Define parameters for hypertuning\n",
    "\n",
    "n_estimators = [25, 35, 45]\n",
    "\n",
    "max_features = [2,4,6]\n",
    "\n",
    "min_samples_split = [10, 20, 40]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "weights = [{1: 3000}, {1:5000}, {1:10000}]\n",
    "\n",
    "max_depth = [5, 10]\n",
    "\n",
    "# Create the grid\n",
    "search_params = {'n_estimators': n_estimators,\n",
    "                 'max_features': max_features,\n",
    "                 'min_samples_split': min_samples_split,\n",
    "                 'bootstrap': bootstrap,\n",
    "                 'criterion': criterion,\n",
    "                 'class_weight': weights,\n",
    "                 'max_depth': max_depth}\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b379b7f",
   "metadata": {},
   "source": [
    "### Noise level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16638c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_1_b = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_1_b.fit(X_1_b_train, y_1_b_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_1_b_v2 = GridSearchCV(model_1_b, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "\n",
    "model_1_b_v2.fit(X_1_b_train, y_1_b_train)\n",
    "\n",
    "best_params = model_1_b_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_1_b_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_1_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 3000}, criterion='entropy', max_depth=10)\n",
    "model_1_b_v3.fit(X_1_b_train, y_1_b_train)\n",
    "\n",
    "scores = cross_val_score(model_1_b_v3, X_1_b, y_1_b, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_1_b_v3, X_1_b, y_1_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823076d4",
   "metadata": {},
   "source": [
    "### Noise level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_2_b = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_2_b.fit(X_2_b_train, y_2_b_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_2_b_v2 = GridSearchCV(model_2_b, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "\n",
    "model_2_b_v2.fit(X_2_b_train, y_2_b_train)\n",
    "\n",
    "best_params = model_2_b_v2.best_params_  \n",
    "\n",
    "# print('Best Parameters', model_1_2_v2.best_params_) <- there's a bug here, do not want to rerun the hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Parameters', model_2_b_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94878be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_2_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 10000}, criterion='entropy', max_depth=10)\n",
    "model_2_b_v3.fit(X_2_b_train, y_2_b_train)\n",
    "\n",
    "scores = cross_val_score(model_2_b_v3, X_2_b, y_2_b, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_2_b_v3, X_2_b, y_2_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc153e",
   "metadata": {},
   "source": [
    "### Noise level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eef090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_3_b = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_3_b.fit(X_3_b_train, y_3_b_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_3_b_v2 = GridSearchCV(model_3_b, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "\n",
    "model_3_b_v2.fit(X_3_b_train, y_3_b_train)\n",
    "\n",
    "best_params = model_3_b_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_3_b_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_3_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 10000}, criterion='entropy', max_depth=10)\n",
    "model_3_b_v3.fit(X_3_b_train, y_3_b_train)\n",
    "\n",
    "scores = cross_val_score(model_3_b_v3, X_3_b, y_3_b, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_3_b_v3, X_3_b, y_3_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dd5a9",
   "metadata": {},
   "source": [
    "## Random Forest 3 - Multiclass with original features\n",
    "#### Features = 'capacity', 'vib1_x', 'vib1_y', 'vib1_z', 'vib2_x', 'vib2_y', 'vib2_z','amp_uni', 'mic1', 'mic2'\n",
    "#### Value = 'type_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Unnamed: 0', 'datetime', 'date', 'time', 'pump', 'pump_cat', 'capacity_cat', 'anomaly', 'anomaly_cat', 'anomaly_binary',\n",
    "       'anomaly_binary_cat', 'type', 'vib1', 'vib1_x', 'vib1_y',\n",
    "       'vib1_z', 'vib2', 'vib2_x', 'vib2_y', 'vib2_z', 'amp1', 'amp2',\n",
    "       'amp_uni', 'mic1', 'mic2',\n",
    "       'vib1_x_noise', 'vib1_with_noise', 'vib1_y_noise','vib1_z_noise', 'vib2_x_noise', 'vib2_with_noise', 'vib2_y_noise', 'vib2_z_noise', 'amp1_noise',\n",
    "       'amp_uni_with_noise', 'amp2_noise', 'mic1_noise', 'mic2_noise']\n",
    "\n",
    "value = 'type_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add016c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's prepare the dataframes for all noise levels\n",
    "\n",
    "for level in levels:\n",
    "    \n",
    "    #read in data\n",
    "    globals()[f'data_{level}'] = pd.read_csv('../data/noisy_data_'+str(level)+'.csv')\n",
    "    \n",
    "    #drop the columns we don't need\n",
    "    globals()[f'df_{level}'] = globals()[f'data_{level}'].drop(drop_list, axis=1)\n",
    "    \n",
    "    #define features and value\n",
    "    globals()[f'X_{level}'] = globals()[f'df_{level}'].drop(value, axis=1)\n",
    "    globals()[f'y_{level}'] = globals()[f'df_{level}'][value]\n",
    "    \n",
    "    #scale the data\n",
    "    scaler = StandardScaler()  \n",
    "    globals()[f'X_{level}'] = scaler.fit_transform(globals()[f'X_{level}']) \n",
    "    \n",
    "    #create train and test datasets\n",
    "    globals()[f'X_{level}_train'], globals()[f'X_{level}_val'], globals()[f'y_{level}_train'], globals()[f'y_{level}_val'] = train_test_split(X, y, test_size = 0.20, random_state = 100)\n",
    "    \n",
    "\n",
    "# Define parameters for hypertuning\n",
    "\n",
    "n_estimators = [25, 35, 45]\n",
    "\n",
    "max_features = [2,4,6]\n",
    "\n",
    "min_samples_split = [10, 20, 40]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "weights = [{1: 1000, 2: 1000, 3: 1000},\n",
    "           {1: 3000, 2: 3000, 3: 3000},\n",
    "           {1: 5000, 2: 5000, 3: 5000},\n",
    "          {1: 10000, 2: 10000, 3: 10000}]\n",
    "\n",
    "max_depth = [5, 10]\n",
    "\n",
    "# Create the random grid\n",
    "search_params = {'n_estimators': n_estimators,\n",
    "                 'max_features': max_features,\n",
    "                 'min_samples_split': min_samples_split,\n",
    "                 'bootstrap': bootstrap,\n",
    "                 'criterion': criterion,\n",
    "                 'class_weight': weights,\n",
    "                 'max_depth': max_depth}\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7d3c5",
   "metadata": {},
   "source": [
    "### Noise level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_1 = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_1.fit(X_1_train, y_1_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_1_v2 = GridSearchCV(model_1, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "\n",
    "model_1_v2.fit(X_1_train, y_1_train)\n",
    "\n",
    "best_params = model_1_v2.best_params_\n",
    "\n",
    "print('Best Parameters', model_1_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_1_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 5000, 2: 5000, 3: 5000}, criterion='entropy', max_depth=10)\n",
    "model_1_v3.fit(X_1_train, y_1_train)\n",
    "\n",
    "scores = cross_val_score(model_1_v3, X_1, y_1, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_1_v3, X_1, y_1, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9e966",
   "metadata": {},
   "source": [
    "### Noise level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaabf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_2 = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_2.fit(X_2_train, y_2_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_2_v2 = GridSearchCV(model_2, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "\n",
    "model_2_v2.fit(X_2_train, y_2_train)\n",
    "\n",
    "best_params = model_2_v2.best_params_\n",
    "\n",
    "print('Best Parameters', model_2_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37434817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_2_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 5000, 2: 5000, 3: 5000}, criterion='entropy', max_depth=10)\n",
    "model_2_v3.fit(X_2_train, y_2_train)\n",
    "\n",
    "scores = cross_val_score(model_2_v3, X_2, y_2, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_2_v3, X_2, y_2, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a66bdd",
   "metadata": {},
   "source": [
    "### Noise level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_3 = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_3.fit(X_3_train, y_3_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_3_v2 = GridSearchCV(model_3, search_params, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "\n",
    "model_3_v2.fit(X_3_train, y_3_train)\n",
    "\n",
    "best_params = model_3_v2.best_params_\n",
    "\n",
    "print('Best Parameters', model_3_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb84692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_3_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 5000, 2: 5000, 3: 5000}, criterion='entropy', max_depth=10)\n",
    "model_3_v3.fit(X_3_train, y_3_train)\n",
    "\n",
    "scores = cross_val_score(model_3_v3, X_3, y_3, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_3_v3, X_3, y_3, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_nonbinary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad8730",
   "metadata": {},
   "source": [
    "## Random Forest 4 - Binary with original features\n",
    "#### Features = 'capacity', 'vib1_x', 'vib1_y', 'vib1_z', 'vib2_x', 'vib2_y', 'vib2_z','amp_uni', 'mic1', 'mic2'\n",
    "#### Value = 'anomaly_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361118e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list_b = ['Unnamed: 0', 'datetime', 'date', 'time', 'pump', 'pump_cat', 'capacity_cat', 'anomaly', 'anomaly_cat', 'type_cat',\n",
    "       'anomaly_binary_cat', 'type', 'vib1', 'vib1_x', 'vib1_y',\n",
    "       'vib1_z', 'vib2', 'vib2_x', 'vib2_y', 'vib2_z', 'amp1', 'amp2',\n",
    "       'amp_uni', 'mic1', 'mic2',\n",
    "       'vib1_x_noise', 'vib1_with_noise', 'vib1_y_noise','vib1_z_noise', 'vib2_x_noise', 'vib2_with_noise', 'vib2_y_noise', 'vib2_z_noise', 'amp1_noise',\n",
    "       'amp_uni_with_noise', 'amp2_noise', 'mic1_noise', 'mic2_noise']\n",
    "\n",
    "value_b = 'anomaly_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's prepare the dataframes for all noise levels\n",
    "\n",
    "for level in levels:\n",
    "    \n",
    "    #drop the columns we don't need\n",
    "    globals()[f'df_{level}_b'] = globals()[f'data_{level}'].drop(drop_list_b, axis=1)\n",
    "    \n",
    "    #define features and value\n",
    "    globals()[f'X_{level}_b'] = globals()[f'df_{level}_b'].drop(value_b, axis=1)\n",
    "    globals()[f'y_{level}_b'] = globals()[f'df_{level}_b'][value_b]\n",
    "    \n",
    "    #scale the data\n",
    "    scaler = StandardScaler()  \n",
    "    globals()[f'X_{level}_b'] = scaler.fit_transform(globals()[f'X_{level}_b']) \n",
    "    \n",
    "    #create train and test datasets\n",
    "    globals()[f'X_{level}_b_train'], globals()[f'X_{level}_b_val'], globals()[f'y_{level}_b_train'], globals()[f'y_{level}_b_val'] = train_test_split(globals()[f'X_{level}_b'], globals()[f'y_{level}_b'], test_size = 0.20, random_state = 100)\n",
    "    \n",
    "\n",
    "# Define parameters for hypertuning\n",
    "\n",
    "n_estimators_b = [25, 35, 45]\n",
    "\n",
    "max_features_b = [2,4,6]\n",
    "\n",
    "min_samples_split_b = [10, 20, 40]\n",
    "\n",
    "bootstrap_b = [True, False]\n",
    "\n",
    "criterion_b = ['gini', 'entropy']\n",
    "\n",
    "weights_b = [{1: 1000},\n",
    "           {1: 3000},\n",
    "           {1: 5000},\n",
    "          {1: 10000}]\n",
    "\n",
    "max_depth_b = [5, 10]\n",
    "\n",
    "# Create the random grid\n",
    "search_params_b = {'n_estimators': n_estimators_b,\n",
    "                 'max_features': max_features_b,\n",
    "                 'min_samples_split': min_samples_split_b,\n",
    "                 'bootstrap': bootstrap_b,\n",
    "                 'criterion': criterion_b,\n",
    "                 'class_weight': weights_b,\n",
    "                 'max_depth': max_depth_b}\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4736a8",
   "metadata": {},
   "source": [
    "### Noise level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_1_b = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_1_b.fit(X_1_b_train, y_1_b_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_1_b_v2 = GridSearchCV(model_1_b, search_params_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "\n",
    "model_1_b_v2.fit(X_1_b_train, y_1_b_train)\n",
    "\n",
    "best_params = model_1_b_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_1_b_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "model_1_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'False', min_samples_split=40, max_features=2, n_estimators = 45, class_weight={1: 5000}, criterion='entropy', max_depth=10)\n",
    "model_1_b_v3.fit(X_1_b_train, y_1_b_train)\n",
    "\n",
    "scores = cross_val_score(model_1_b_v3, X_1_b, y_1_b, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_1_b_v3, X_1_b, y_1_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52669a4",
   "metadata": {},
   "source": [
    "### Noise level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fadd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_2_b = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_2_b.fit(X_2_b_train, y_2_b_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_2_b_v2 = GridSearchCV(model_2_b, search_params_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "\n",
    "model_2_b_v2.fit(X_2_b_train, y_2_b_train)\n",
    "\n",
    "best_params = model_2_b_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_2_b_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36087037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "#model_2_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=20, max_features=2, n_estimators = 45, class_weight={1: 1000}, criterion='entropy', max_depth=10)\n",
    "model_2_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'False', min_samples_split=40, max_features=2, n_estimators = 45, class_weight={1: 5000}, criterion='entropy', max_depth=10)\n",
    "model_2_b_v3.fit(X_2_b_train, y_2_b_train)\n",
    "\n",
    "scores = cross_val_score(model_2_b_v3, X_2_b, y_2_b, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_2_b_v3, X_2_b, y_2_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be19c4e",
   "metadata": {},
   "source": [
    "### Noise level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162889cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model_3_b = RandomForestClassifier(random_state = 100)\n",
    "\n",
    "model_3_b.fit(X_3_b_train, y_3_b_train)\n",
    "\n",
    "\n",
    "#perform hypertuning\n",
    "model_3_b_v2 = GridSearchCV(model_3_b, search_params_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "\n",
    "model_3_b_v2.fit(X_3_b_train, y_3_b_train)\n",
    "\n",
    "best_params = model_3_b_v2.best_params_  \n",
    "\n",
    "print('Best Parameters', model_3_b_v2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the hypetuned model\n",
    "\n",
    "#model_3_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'True', min_samples_split=40, max_features=2, n_estimators = 35, class_weight={1: 3000}, criterion='entropy', max_depth=10)\n",
    "model_3_b_v3 = RandomForestClassifier(random_state = 100, bootstrap = 'False', min_samples_split=40, max_features=2, n_estimators = 45, class_weight={1: 5000}, criterion='entropy', max_depth=10)\n",
    "\n",
    "model_3_b_v3.fit(X_3_b_train, y_3_b_train)\n",
    "\n",
    "scores = cross_val_score(model_3_b_v3, X_3_b, y_3_b, cv=cv, n_jobs=-1)\n",
    "print('Cross Validation accuracy scores: %s' % scores)\n",
    "print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))\n",
    "print('\\n')\n",
    "\n",
    "scores = cross_val_score(model_3_b_v3, X_3_b, y_3_b, cv=cv, n_jobs=-1, scoring=make_scorer(cedric_metric_binary, greater_is_better=True))\n",
    "print('Cross Validation custom metric scores: %s' % scores)\n",
    "print('Cross Validation custom metric: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b57660",
   "metadata": {},
   "source": [
    "## In case of noise levels 2 and 3 parameters hypertuned for noise level 1 give better results than parameters hypertuned for noise levels 2 and 3 accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fc0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
